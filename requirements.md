Local video-to-clips pipeline running in Docker with Python backend and React frontend where user selects a long-form video file from local disk (2+ hour podcasts/meetings), the system uses local FFmpeg for format conversion and local Whisper model (optimized for M1 16GB) for transcription with word-level timestamps, then sends transcript to Gemini 2.5 Pro or 3.0 Preview via OpenRouter API to find engaging/hooking moments with auto-determined clip length (13-60 sec based on complete phrases), recommended moments with timestamps are displayed on frontend where user can also manually select any custom time range, user chooses frame template (1-frame: single vertical 9:16 crop of one speaker, 2-frame: two rectangles stacked vertically for 2 speakers or speaker+screen, 3-frame: 2 speakers side-by-side on top + screen below), interactive frame position editor on frontend where user drags crop areas on video preview and switches templates, upon confirmation FFmpeg extracts video chunks from selected regions/timeframes and composites them with single audio track into final 9:16 vertical video, subtitles generated in .ass format with one-word-at-a-time display style (social media karaoke style) with customizable font/color/size/animation and user-selectable position on screen then hardcoded into final video, projects are saved locally with full history, interface in English but videos can be any language (Russian/English/etc), system designed for local execution with Docker handling FFmpeg and Whisper model installation with proper volume mapping for accessing local video files.